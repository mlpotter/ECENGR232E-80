---
title: "UCLA ECE 232E: Project 1"
author: "Sam Anderson and Michael Potter"
date: "6/24/2021"
output: 
  html_notebook: default
  pdf_document: default
---

```{r include= FALSE}
library(igraph)
library(Matrix)
library(pracma)
library(knitr)

knit_print.data.frame <- lemon_print
#library(ggplot2)
```

# Part 1: Generating random networks
## Question 1: Generating Erdős–Rényi networks

The Erdős–Rényi (ER) model generates a random undirected network $G$ using the function $G(n,p)$ where $n$ is the number of nodes and $p$ is the independent probability that an edge will exist between any pair of nodes in the network. 

### (a) Creating undirected ER networks

We generate five undirected networks using the ER model: $G(1000,0.003)$, $G(1000,0.004)$, $G(1000,0.01)$, $G(1000,0.05)$, and $G(1000,0.1)$. For a given node, its degree is theoretically distributed according to a [binomial distribution](/https://en.wikipedia.org/wiki/Binomial_distribution) $B(n-1,p)$. Therefore, the theoretical degree distribution is:
$$P(\text{degree} = k)=\binom{n-1}{k}p^k(1-p)^{n-1-k}$$
This is because the degree of a node is the number of successes after $n-1$ independent tests (whether it will share an edge with each of the other $n-1$ nodes) each with an independent probability of success $p$. Therefore, the theoretical mean degree of a node is  $(n-1)p$ and the theoretical variance in the degree distribution is $(n-1)p(1-p)$. 

For each network we generate, the degree distribution will be plotted along with the experimental mean degree and degree distribution variance, the theoretical values, and the error between experimental and theoretical values. 

```{r, results='asis'}
set.seed(0) #set R's random number generator seed

n = 1000 #set the number of nodes
p = c(0.003,0.004,0.01,0.05,0.1) #list of p values to be used


#initialize arrays
mean_exp = vector()
mean_theo = vector()
mean_error = vector()
var_exp = vector()
var_theo = vector()
var_error = vector() 

for(i in 1:length(p)){
  g = sample_gnp(n,p[i]) #generate a graph using the ER Model
  dist = degree_distribution(g) #degree distribution
  degrees = degree(g) #degree of each node
  
  plot(dist,xlab="Degree",ylab="Frequency")
  title(main=paste("G(1000,",p[i],") Degree Distribution",sep=""))
  cat('\n\n<!-- -->\n\n')
  
  mean_exp[i] = mean(degrees) #experimental mean degree
  mean_theo[i] = (n-1)*p[i] #theoretical mean degree
  mean_error[i] = mean_theo[i]-mean_exp[i] #error
  
  var_exp[i] = var(degrees) #experimental degree distribution variance
  var_theo[i] = (n-1)*p[i]*(1-p[i]) #theoretical degree distribution variance
  var_error[i] = var_theo[i]-var_exp[i] #error
  
  results <- data.frame(
    value_type = c("experimental","theoretical","error"),
    mean = c(mean_exp[i],mean_theo[i],mean_error[i]),
    variance = c(var_exp[i],var_theo[i],var_error[i])
  )
  
  print(kable(results, digits=2, caption = paste("Degree Statistics for G(1000,",p[i],")",sep="")))
  cat('\n\n<!-- -->\n\n')
}

```

### (b) Determining connectedness
A network generated by the ER model is not necessarily connected. For example, the probability of a given node being isolated is $(1-p)^{n-1}$ which is non-zero for $p<1$. To estimate the probability that a given network $G(n,p)$ is connected, we will generate $m=1000$ independent random networks for each of the five values of $p$ tested above and record the fraction of those networks that are connected. 

```{r 1b1}
m = 1000 #number of samples for the numerical estimation
connected = matrix(nrow=m,ncol=length(p)) #store results of tests
num_connected = c(1:length(p)) #store number of connected graphs

for(i in 1:length(p)){
  for(j in 1:m){
    g = sample_gnp(n,p[i])
    if(is_connected(g)){
      connected[j,i]=1
    } 
    else{
      connected[j,i]=0
    }
  }
  num_connected[i]=sum(connected[,i])
}

graph_names = c(1:length(p))
for(i in 1:length(p)){
  graph_names[i]=paste("G(",n,",",p[i],")",sep="")
}

results <- data.frame(
    graph = graph_names,
    number_of_samples = rep(m,length(p)),
    number_of_connected_graphs = num_connected,
    probability_of_connectedness = num_connected/m
  )
  
print(kable(results, digits=2, caption = "Numerical estimation of the probability of G(1000,p) being connected for various p values"))


```

Below we identify the giant connected component (GCC) for one instance of $G(1000,0.004)$ and determine its diameter. For this example, the GCC contains 982 nodes and has a diameter of 11.
```{r 1b2}
set.seed(0)
g = sample_gnp(1000,0.004) #generate graph
clusters = clusters(g) #identify connected components
gcc_nodes = which(clusters[[1]] %in% 1) #create a list of the vertices in the graph's GCC

#measure the diameter of the GCC
gcc = induced_subgraph(g,gcc_nodes)
print(paste("The GCC of G(1000,0.004) contains ",length(gcc_nodes)," nodes and has a diameter of ",diameter(gcc),".",sep=""))

```

### (c) Normalized GCC size vs. $p$
We develop a scatter plot of $p$ vs. normalized GCC size for $G(1000,p)$. We will sweep through $100$ increments of $p$ from $p=0$ to $p_{max}=0.007$, and at each increment of $p$ sample $100$ iterations of $G(1000,p)$.

```{r 1c1}
set.seed(0)
n = 1000
p_max = 0.007 
k = 100 #number of p increments
gcc_p_sweep = matrix(nrow=2,ncol=100*k) #matrix for holding results

for(i in 1:100){
  p = (i-1)*p_max/(k-1) #set p value
  for(j in 1:k){
    index = (i-1)*k+j-1
    gcc_p_sweep[1,index] = p #store p value
    g = sample_gnp(n,p)
    gcc_nodes = which(clusters(g)[[1]] %in% 1)
    gcc_p_sweep[2,index] = length(gcc_nodes)/n #store normalized gcc size
  }
}

#calculate average GCC size for each p increment
gcc_avg = matrix(nrow=2,ncol=k)
for(i in 1:k){
  p = (i-1)*p_max/(k-1)
  gcc_avg[1,i] = p
  gcc_avg[2,i] = mean(gcc_p_sweep[2,which(gcc_p_sweep[1,] %in% p)])
}


plot(gcc_p_sweep[1,],gcc_p_sweep[2,],xlab="p",ylab="Normalized GCC Size")
title(main="Normalized GCC Size vs. p for G(1000,p)")
lines(gcc_avg[1,],gcc_avg[2,],col="red")
legend(0.005, 0.4, legend=c("Sampled GCC Sizes","Average GCC Size"),col=c("black","red"), lty=1:1,cex=0.8)

#questions (i),(ii)

```
A GCC begins to emerge at $p=0.001=\frac{1}{n}$, using a definition of a normalized GCC size of at least $0.1$ as the criterion for emergence. While this value of $0.1$ is arbitrary, the normalized GCC size is highly positively correlated with $p$ at or above this value of approximately $p=\frac{1}{n}$. 

*Discussion of theory from lectures required.*

### (d) GCC size vs. $c = n \times p$
text text

```{r 1d1}
c_list = c(0.5,1,1.1,1.2,1.3)
for(i in 1:length(c_list)){
  set.seed(0)
  c = c_list[i]
  n_min = 100
  n_max = 10000
  gcc_n_sweep = matrix(nrow=2,ncol=n_max-n_min+1)

  for(n in n_min:n_max){
    index = n - n_min +1
    g = sample_gnp(n,c/n)
    gcc_nodes = which(clusters(g)[[1]] %in% 1)
    gcc_n_sweep[1,index]=n
    gcc_n_sweep[2,index]=length(gcc_nodes)
  }

  plot(gcc_n_sweep[1,],gcc_n_sweep[2,],xlab="n",ylab="GCC size")
  title(main = paste("GCC size vs. n, c = p*n =",c))
}


```

## Question 2: Preferential attachment networks
The preferential attachment (PA) model generates a network by adding each node sequentially and connecting it to a pre-defined number of pre-existing nodes, $m$, according to a probability distribution. 

### (a) Creating an undirected PA network with $n=1000, m=1$
A PA network with $m=1$ is always connected since no matter what, every node added to the network is connected to a pre-existing node. By induction, the entire network is connected. 
```{r 2a1}
set.seed(0)
n = 1000
g1 <- sample_pa(n, m=1, directed = FALSE)

```
### (b) Finding community structure using the fast greedy method
```{r 2b1}
communities = cluster_fast_greedy(g1)

print(sizes(communities))
print(paste("The modularity is ",modularity(communities),sep=""))
```
### (c) Determining modularity for a PA network with $n=10000, m=1$
The modularity is found to be higher at $0.98$ for $n=10000$ compared to $0.93$ for $n=1000$.
```{r 2c1}
set.seed(0)
n = 10000
g2 <- sample_pa(n, m=1, directed = FALSE)
communities = cluster_fast_greedy(g2)
print(paste("The modularity is ",modularity(communities),sep=""))
```
### (d) Plotting degree distribution for both PA networks
```{r 2d1}
set.seed(0)
g1 <- sample_pa(1000, m=1, directed = FALSE)
g2 <- sample_pa(10000, m=1, directed = FALSE)
dist1_y = degree_distribution(g1)
dist1_x = c(1:length(dist1_y))
dist2_y = degree_distribution(g2)
dist2_x = c(1:length(dist2_y))

plot(log(dist1_x),log(dist1_y),xlab="log(k)",ylab="log(p_k)")
points(log(dist2_x),log(dist2_y),col="red")
title(main = "Log-Log of Degree Distribution for PA Networks")
legend(2,-1,legend=c("n=1000, m=1","n=10000, m=1"),lty=1:1,col=c("black","red"))

degree_data = data.frame(
  k = c(dist1_x,dist2_x),
  p_k = c(dist1_y,dist2_y)
)

lm_p_k = lm(p_k~k, data = degree_data)

print(paste("The slope of the p_k vs. k curve is estimated to be ",lm_p_k$coefficients[2]," by linear regression.",sep=""))
```
### (e) Plotting the degree distribution of a random node's neighbour
*insert conclusions*
```{r 2e1}
set.seed(0)

g1_n = 1000
g2_n = 10000

g1 <- sample_pa(g1_n, m=1, directed = FALSE)
g2 <- sample_pa(g2_n, m=1, directed = FALSE)


#define a function that picks a random node graph, then picks a random neighbour and returns its vertex id
j_degree <- function(g){
  n = gorder(g)
  i = sample(V(g),1)
  sample(neighbors(g,i),1)
  }

iterations = 1000 #number of iterations
g1_j = c()
g2_j = c()

for(s in 1:iterations){
  g1_j = c(g1_j,j_degree(g1))
  g2_j = c(g2_j,j_degree(g2))
}

g1_j_dist_y = degree_distribution(induced_subgraph(g1,g1_j))
g1_j_dist_x = c(1:length(g1_j_dist_y))
g2_j_dist_y = degree_distribution(induced_subgraph(g2,g2_j))
g2_j_dist_x = c(1:length(g2_j_dist_y))

plot(log(g1_j_dist_x),log(g1_j_dist_y),xlab="log(k)",ylab="log(p_k)")
points(log(g2_j_dist_x),log(g2_j_dist_y),col="red")
title(main = "Log-Log of j node degree distribution for PA Networks")
legend(2,-1,legend=c("n=1000, m=1","n=10000, m=1"),lty=1:1,col=c("black","red"))

degree_data = data.frame(
  k = c(g1_j_dist_x,g2_j_dist_x),
  p_k = c(g1_j_dist_y,g2_j_dist_y)
)

lm_p_k = lm(p_k~k, data = degree_data)

print(paste("The slope of the p_k vs. k curve is estimated to be ",lm_p_k$coefficients[2]," by linear regression.",sep=""))
```